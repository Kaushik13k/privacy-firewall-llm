# ğŸ” Privacy Firewall for LLMs

## Overview
As language models enter production environments, ensuring privacy, safety, and control becomes critical. This project is a modular AI privacy firewall â€” a middleware that sits between your application and an LLM (like OpenAI, Gemini, etc.) to enforce:


| Feature                     | Description                                                              |
| --------------------------- | ------------------------------------------------------------------------ |
|PII Detection | Detects and optionally redacts PII from user prompts                     |
|Session Context Memory   | Maintains per-session rolling context based on token usage               |
|Prompt Injection Guard   | Detects common prompt injection attack phrases                           |
|Abuse Pattern Monitoring | Tracks suspicious users across time windows (rate + semantic thresholds) |
|Risk Scoring Engine      | Assigns scores based on length, keywords, and detected PII               |
|Token Budget Manager     | Ensures token-limited session memory per user                            |


## Architecture

                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   User / Frontend  â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   FastAPI Proxy     â”‚
                       â”‚ /ask â†’ LLM Backend  â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚              â”‚                        â”‚            â”‚
      PII Detector  ğŸ›¡ Prompt Guard       Abuse Detector  ğŸ§  Memory Store
          â”‚              â”‚                        â”‚            â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â–¼
                          Risk Scoring Engine
                                  â”¬
                                  â–¼
                        LLM (OpenAI / Gemini)
                                  â”¬
                                  â–¼
                       Streamlit Dashboard (Live Logs)


## Project Structure
```
.
â””â”€â”€ app
    â”œâ”€â”€ api
    â”‚   â”œâ”€â”€ main.py
    â”‚   â”œâ”€â”€ middlewares
    â”‚   â”‚   â””â”€â”€ pii_firewall.py
    â”‚   â”œâ”€â”€ rate_limiter.py
    â”‚   â””â”€â”€ routers
    â”‚       â”œâ”€â”€ chat.py
    â”‚       â””â”€â”€ health.py
    â”œâ”€â”€ firewall
    â”‚   â”œâ”€â”€ abuse_detector.py
    â”‚   â”œâ”€â”€ prompt_injection.py
    â”‚   â”œâ”€â”€ scorer.py
    â”‚   â”œâ”€â”€ session_memory.py
    â”‚   â””â”€â”€ token_utils.py
    â”œâ”€â”€ payloads.md
    â”œâ”€â”€ README
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ streamlit_dashboard
    â”‚   â””â”€â”€ app.py
    â””â”€â”€ tests
        â””â”€â”€ __init__.py
```

## Getting Started
1.  ### Set up environment
  ```
  python -m venv venv
  source venv/bin/activate
  pip install -r requirements.txt
  ```
2. ### Install Redis
  ```
  brew install redis
  ```
3. ### Start the backend API
  ```
  cd app
  PYTHONPATH=. uvicorn api.main:app --reload
  ```
4. ### Start the dashboard
  ```
  cd app/streamlit_dashboard
  streamlit run app.py
  ```